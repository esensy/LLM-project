# main.py

import json, os, sys, torch

ROOT_PATH = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(ROOT_PATH)
if PARENT_DIR not in sys.path:
  sys.path.insert(0, PARENT_DIR)

from huggingface_hub import notebook_login
from langchain_openai import ChatOpenAI
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.load import dumps, loads

from ì œì•ˆìš”ì²­ì„œ.load_embedding_model import load_embedding_model
from ì œì•ˆìš”ì²­ì„œ.config import (EMBEDDING_MODEL_NAME, METADATA_PATH, DOCS_PATH, 
                          CHROMA_METADATA_PATH, CHROMA_DOCUMENT_PATH, CHAT_MODEL_NAME, 
                          CHAT_TEMPERATURE, CHAT_N_RESPONSES, TOP_K_RETRIEVAL, 
                          PROJECT_ROOT, SAVE_PATH)
from ì œì•ˆìš”ì²­ì„œ.utils.utils import clean_filename, print_divider, load_metadata, load_documents_pre_chunked
from ì œì•ˆìš”ì²­ì„œ.database.database_utils import create_metadata_db, create_document_db 
from ì œì•ˆìš”ì²­ì„œ.query.query import process_query

# ë©”ì¸ í•¨ìˆ˜.
def main():
  # 1. CUDA ì‚¬ìš©, Hugging Face ë¡œê·¸ì¸, OpenAI API í‚¤ ì„¤ì •.
  notebook_login()
  print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}.")
  OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") # OpenAI key ì„¤ì •.
  
  if not OPENAI_API_KEY:
    OPENAI_API_KEY = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    print(f"Warning: OPENAI_API_KEY environment variable not found. Using manually entered key.")
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

  # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ.
  print("ğŸŒŸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘....")
  embeddings = load_embedding_model(EMBEDDING_MODEL_NAME)

  # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ.
  print("\nğŸª„ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘....")
  metadata_df = load_metadata(METADATA_PATH)

  # 4. ë¬¸ì„œ ë¡œë“œ.
  print("\nğŸ† ë¬¸ì„œ ë¡œë“œ ì¤‘....")
  pre_chunked_data = load_documents_pre_chunked(DOCS_PATH)

  # 5. ë©”íƒ€ë°ì´í„° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±.
  print("\nâœ¨ ë©”íƒ€ë°ì´í„° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘....")
  metadata_db = create_metadata_db(metadata_df, embeddings, CHROMA_METADATA_PATH)

  # 6. ë¬¸ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±.
  print("\nğŸ‰ ë¬¸ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘....")
  document_db, bm25_retriever = create_document_db(pre_chunked_data, metadata_df, embeddings, CHROMA_DOCUMENT_PATH)

  # 7. ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤.
  print("\n=== ê³µê³µì…ì°° ê³µê³  ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ì‹œì‘ ===")
  print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

  # 8. ChatGPT ëª¨ë¸ ì„¤ì •.
  chat = ChatOpenAI(
      model = CHAT_MODEL_NAME,
      temperature = CHAT_TEMPERATURE,
      n = CHAT_N_RESPONSES,
      openai_api_key = os.environ["OPENAI_API_KEY"]
  )

  chat_history = [] # ëŒ€í™” ê¸°ë¡.
  all_results = []  # ê²°ê³¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸.
  while True:
    print_divider()
    query = input("\nì§ˆë¬¸: ")
    if query.lower() == "exit":
      break

    # ì§ˆë¬¸ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±.
    result = process_query(chat, query, metadata_db, document_db, bm25_retriever, embeddings, metadata_df, top_k = TOP_K_RETRIEVAL, re_rank = True, chat_history = chat_history)

    # ê²°ê³¼.
    all_results.append({query: result})

    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸.
    chat_history.extend([HumanMessage(content = query), AIMessage(content = result["response"])]) # "human" ë° "ai" í‚¤ì›Œë“œì„.

    # ì ë‹¹íˆ ê¸°ë¡.
    chat_history = chat_history[-6:]

    # ì‘ë‹µ ì¶œë ¥.
    print("\n\033[91m=== ì‘ë‹µ ===\033[0m")
    print(result["response"])

    # ê´€ë ¨ íŒŒì¼ ì •ë³´ ì¶œë ¥.
    print("\n\033[93m=== ê´€ë ¨ íŒŒì¼ ===\033[0m")
    for file in result["relevant_files"]:
      print(f"- {file}")

    # ë©”ì¸ í•¨ìˆ˜ì—ì„œ ê²°ê³¼ ì¶œë ¥ ì‹œ ë©”íƒ€ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ë„ í™•ì¸.
    print("\n\033[95m=== ë©”íƒ€ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ===\033[0m")
    if "metadata_context" in result:
      metadata_context = result["metadata_context"]
      print(metadata_context)
    else:
      print("ë©”íƒ€ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ì°¸ì¡° ì²­í¬ ì¶œë ¥ ì½”ë“œ ì¶”ê°€.
    print("\n\033[94m=== ì°¸ì¡°í•œ ì²­í¬ ì •ë³´ ===\033[0m")
    if result["referenced_chunks"]:
      for i, chunk in enumerate(result["referenced_chunks"], 1):
        print(f"\n\033[92m--- ì°¸ì¡° ì²­í¬ {i} ---\033[0m")
        print(f"ì‚¬ì—…ëª…: {chunk['ì‚¬ì—…ëª…']}")
        print(f"ë°œì£¼ê¸°ê´€: {chunk['ë°œì£¼ê¸°ê´€']}")
        print(f"ìœ ì‚¬ë„ ì ìˆ˜: {chunk['similarity_score']:.4f}")

        # ì²­í¬ ë‚´ìš© ìš”ì•½ ì¶œë ¥ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì¶œë ¥).
        content = chunk["content"]
        if len(content) > 500:
          print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:500]}....")
          print(f"(ì´ {len(content)}ì).")
        else:
          print(f"ë‚´ìš©: {content}.")
    else:
      print("ì°¸ì¡°í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

  # íŒŒì¼ì— ì €ì¥í•˜ê¸°.
  # ê°ì²´ë¥¼ JSON ë¬¸ìì—­ë¡œ ë¨¼ì € ë³€í™˜.
  json_str = dumps(all_results)
  with open(SAVE_PATH, "w", encoding = "utf-8") as f:
    f.write(json_str)

  print(f"\nëª¨ë“  ì¿¼ë¦¬ ê²°ê³¼ {SAVE_PATH} ê²½ë¡œë¡œ ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
  main()
 